+++
title = "微服务最佳实践"
date = "2021-02-12T15:03:15+08:00"
author = ""
authorTwitter = "" #do not include @
cover = ""
tags = ["微服务"]
keywords = ["微服务"]
description = ""
showFullContent = false
+++

# 微服务最佳实践
本文记录了工作当中积累的一些想法

## 一定要做到的点
- 每个请求一定要有 `request-id`，一般由调用方生成，如果你是在推动一个遗留项目，那么可以在网关或者`AOP`层去生成它，这样才能够在繁杂的日志中找到特定的请求信息。建议由 `uuid` 或者 雪花算法生成。
- 请求日志和错误日志分离，这么是因为这两者具有完全不同的关注点，且前者格式相对固定。分离他们可以在清洗日志时使用不同的格式。
- 当请求发生错误时（无论是预期内的还是预期外）都应该体现在日志当中
- 如果你们的系统引入了调用链追踪，那么可以使用 `request-id` 作为 `trace-id`。
- 对于每一个资源的操作都应有反馈，比如删除不存在的资源或者创建同名的资源，我们都应该返回特定的错误码，通常可以使用三类: `NotFoundError`, `ConflictError`, `InternalError`，`ValidationError`
- 网关和微服务打印日志建议分开侧重点，网关应该打印协议层的细节，比如 http header, http code, cookie等，而微服务应该打印 应用Code, 请求参数，响应参数。这样可以在网关统计到后端微服务有多少是因为转发时的网络问题而导致访问失败，多少又是因为应用服务内的问题而失败。
- 系统和服务都应该有明确的边界，服务边界很容易理解，就是一个独立的部署单元，系统的边界是指对所有系统外的服务的交互都应该收口，这样才能更好的进行流量治理。理想情况下我们可以通过网络策略来形成物理边界，比如k8s的namespace。但是在真实的情况中可能你要面对很多遗留服务，它们已经通过IP地址或者服务发现互相访问了，直接去改造它们成本非常大，那么你可以在逻辑上形成边界，比如提炼一个新的概念对象，将现存的服务都分门别类，然后再请求上就可以区分出流量到底来自外部还是内部。不过值得注意的是，物理边界虽然理想，但是在做异地多活时可能会面临一些挑战,比如你使用namespace来作为网络边界，那么你需要把不同地区和机房的机器都包含到一个集群中才能满足不同程度的容灾需求。
- 谨慎对待 Error 和 Warning 级别的日志，它们可以用于反馈当前服务的健康度
- 乐观锁可以使用最后更新时间或者http标准下的ETag
- 请求时一定要注意超时时间，尤其是调用链路比较长时，要保证 client 和 server 的context声明周期一致

## 框架选择
- 选择什么框架其实不是很重要，建议把代码构建成框架无关的形式，一般情况下可以使用一些抽象中间件完成，比如 [droplet](https://github.com/ShiningRush/droplet)

## 分布式原语
可以参考 Bilgin Ibryam 的博客 [Top 10 must-know Kubernetes design patterns](http://www.ofbizian.com/2020/05/top-10-must-know-kubernetes-design.html)

## Mono-repo or Multi-repo
取决于软件的场景和性质，自己的经验：
- 当仓库用于存储一个紧密耦合的产品时，如 grafana,prometheus 的前后端，k8s 的kubelet, scheduler 等，为了保证各个组件间的兼容性和产品版本的一致性，会放在同一个仓库中
- 当仓库用于存储业务服务时，建议使用 multi-repo 因为你不知道业务的边界在哪里，什么时候会膨胀，谷歌虽然整个公司使用一个仓库，但其实他们使用了一些技术手段来保证只迁出所需部分，同时在基础设施上下足了功夫，比如提高 DevOps 的效率，权限控制，急速膨胀的commit记录等
- 当仓库用于存储公共库，建议使用 multi-repo ，我相信谁都不会因为引用了一个库而导致把全公司的项目引用都拉进来的。

## 缓存
接口缓存的最佳策略：先更新后删除，虽然有部分场景会导致保存旧数据，但是很苛刻。除了这种策略外，如果某个接口受太多数据源影响，可以采用资源时间戳的方式来解决，将最新资源更新的时间戳存储在一个资源集的Key里面，然后比对上次缓存的结果。
https://juejin.cn/post/6844903665845665805